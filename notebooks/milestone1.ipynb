{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose:\n",
    "In this notebook, we will attempt to download a data dump containing daily rainfall over NSW, Australia dataset found on [figshare](https://figshare.com/articles/dataset/Daily_rainfall_over_NSW_Australia/14096681). This data dump is comprised of different files contained in a 776.4 MB compressed format. This size of the uncompressed data dump is approximately 6.6 GB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import pandas as pd\n",
    "from memory_profiler import memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the API request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [requests library](https://docs.python-requests.org/en/master/) to generate an http request call to the [figshare API](https://docs.figshare.com/). Specifically, we will make a call to the `/articles` endpoint to retrieve information about the article of interest including the url we need to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id = 14096681\n",
    "base_url = 'https://api.figshare.com/v2'\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "endpoint = f'/articles/{article_id}'\n",
    "output_directory = \"rainfall/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'defined_type_name': 'dataset',\n",
       " 'embargo_date': None,\n",
       " 'citation': 'Beuzen, Tomas (2021): Daily rainfall over NSW, Australia. figshare. Dataset. https://doi.org/10.6084/m9.figshare.14096681.v3',\n",
       " 'url_private_api': 'https://api.figshare.com/v2/account/articles/14096681',\n",
       " 'embargo_reason': '',\n",
       " 'references': ['https://www.wcrp-climate.org/wgcm-cmip/wgcm-cmip6',\n",
       "  'https://pangeo-data.github.io/pangeo-cmip6-cloud/',\n",
       "  'https://www.longpaddock.qld.gov.au/silo/'],\n",
       " 'funding_list': [],\n",
       " 'url_public_api': 'https://api.figshare.com/v2/articles/14096681',\n",
       " 'id': 14096681,\n",
       " 'custom_fields': [],\n",
       " 'size': 814109773,\n",
       " 'metadata_reason': '',\n",
       " 'funding': None,\n",
       " 'figshare_url': 'https://figshare.com/articles/dataset/Daily_rainfall_over_NSW_Australia/14096681',\n",
       " 'embargo_type': 'file',\n",
       " 'title': 'Daily rainfall over NSW, Australia',\n",
       " 'defined_type': 3,\n",
       " 'embargo_options': [],\n",
       " 'is_embargoed': False,\n",
       " 'version': 3,\n",
       " 'embargo_title': '',\n",
       " 'url_public_html': 'https://figshare.com/articles/dataset/Daily_rainfall_over_NSW_Australia/14096681',\n",
       " 'confidential_reason': '',\n",
       " 'resource_doi': None,\n",
       " 'files': [{'is_link_only': False,\n",
       "   'name': 'daily_rainfall_2014.png',\n",
       "   'supplied_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "   'computed_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "   'id': 26579150,\n",
       "   'download_url': 'https://ndownloader.figshare.com/files/26579150',\n",
       "   'size': 58863},\n",
       "  {'is_link_only': False,\n",
       "   'name': 'environment.yml',\n",
       "   'supplied_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "   'computed_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "   'id': 26579171,\n",
       "   'download_url': 'https://ndownloader.figshare.com/files/26579171',\n",
       "   'size': 192},\n",
       "  {'is_link_only': False,\n",
       "   'name': 'README.md',\n",
       "   'supplied_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "   'computed_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "   'id': 26586554,\n",
       "   'download_url': 'https://ndownloader.figshare.com/files/26586554',\n",
       "   'size': 5422},\n",
       "  {'is_link_only': False,\n",
       "   'name': 'data.zip',\n",
       "   'supplied_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "   'computed_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "   'id': 26766812,\n",
       "   'download_url': 'https://ndownloader.figshare.com/files/26766812',\n",
       "   'size': 814041183},\n",
       "  {'is_link_only': False,\n",
       "   'name': 'get_data.py',\n",
       "   'supplied_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "   'computed_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "   'id': 26766815,\n",
       "   'download_url': 'https://ndownloader.figshare.com/files/26766815',\n",
       "   'size': 4113}],\n",
       " 'handle': '',\n",
       " 'description': '<div><div><div>This data dump contains modelled and observed daily rainfall data over NSW, Australia, spanning 1889 - 2014. Please see README.md for more information.</div></div></div>',\n",
       " 'tags': ['rainfall data', 'Python'],\n",
       " 'timeline': {'revision': '2021-03-29T20:56:29',\n",
       "  'firstOnline': '2021-02-23T16:57:52',\n",
       "  'posted': '2021-03-29T20:56:29'},\n",
       " 'url_private_html': 'https://figshare.com/account/articles/14096681',\n",
       " 'published_date': '2021-03-29T20:56:29Z',\n",
       " 'modified_date': '2021-03-29T20:56:31Z',\n",
       " 'authors': [{'url_name': 'Tomas_Beuzen',\n",
       "   'is_active': True,\n",
       "   'id': 10182999,\n",
       "   'full_name': 'Tomas Beuzen',\n",
       "   'orcid_id': ''}],\n",
       " 'is_public': True,\n",
       " 'categories': [{'parent_id': 33, 'id': 78, 'title': 'Atmospheric Sciences'},\n",
       "  {'parent_id': 33, 'id': 204, 'title': 'Climate Change Processes'},\n",
       "  {'parent_id': 33, 'id': 144, 'title': 'Climate Science'},\n",
       "  {'parent_id': 33,\n",
       "   'id': 205,\n",
       "   'title': 'Climatology (excl. Climate Change Processes)'},\n",
       "  {'parent_id': 33, 'id': 260, 'title': 'Environmental Management'},\n",
       "  {'parent_id': 33, 'id': 34, 'title': 'Environmental Science'},\n",
       "  {'parent_id': 33, 'id': 80, 'title': 'Hydrology'},\n",
       "  {'parent_id': 33, 'id': 207, 'title': 'Meteorology'}],\n",
       " 'thumb': 'https://s3-eu-west-1.amazonaws.com/pfigshare-u-previews/26579150/thumb.png',\n",
       " 'is_confidential': False,\n",
       " 'doi': '10.6084/m9.figshare.14096681.v3',\n",
       " 'has_linked_file': False,\n",
       " 'license': {'url': 'https://creativecommons.org/licenses/by/4.0/',\n",
       "  'name': 'CC BY 4.0',\n",
       "  'value': 1},\n",
       " 'url': 'https://api.figshare.com/v2/articles/14096681',\n",
       " 'resource_title': None,\n",
       " 'status': 'public',\n",
       " 'created_date': '2021-03-29T20:56:29Z',\n",
       " 'group_id': None,\n",
       " 'is_metadata_record': False}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.request(\"GET\", base_url+endpoint, headers=headers)\n",
    "data = json.loads(response.text)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response above contains a `data` json key which is of interest of us. It lists the files corresponding to the article along with their download urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'is_link_only': False,\n",
       "  'name': 'daily_rainfall_2014.png',\n",
       "  'supplied_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "  'computed_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "  'id': 26579150,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579150',\n",
       "  'size': 58863},\n",
       " {'is_link_only': False,\n",
       "  'name': 'environment.yml',\n",
       "  'supplied_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "  'computed_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "  'id': 26579171,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579171',\n",
       "  'size': 192},\n",
       " {'is_link_only': False,\n",
       "  'name': 'README.md',\n",
       "  'supplied_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "  'computed_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "  'id': 26586554,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26586554',\n",
       "  'size': 5422},\n",
       " {'is_link_only': False,\n",
       "  'name': 'data.zip',\n",
       "  'supplied_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "  'computed_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "  'id': 26766812,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766812',\n",
       "  'size': 814041183},\n",
       " {'is_link_only': False,\n",
       "  'name': 'get_data.py',\n",
       "  'supplied_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "  'computed_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "  'id': 26766815,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766815',\n",
       "  'size': 4113}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = data[\"files\"]           \n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data dump is `data.zip` which can be retreived as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the file of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "files_to_dl = [\"data.zip\"] \n",
    "for file in files:\n",
    "    if file[\"name\"] in files_to_dl:\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        urlretrieve(file[\"download_url\"], output_directory + file[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the output above, the single act of downloading the data dump to the local machine took around 2mins 9s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of the download operation across 4 different machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|           | CPU               | RAM   | HD         | Operation time |\n",
    "|-----------|-------------------|-------|------------|----------------|\n",
    "| Machine 1 | i5-4460 @ 3.20Ghz | 10 GB | 1 TB SSD | 2 min 9 sec   |\n",
    "| Machine 2 | i7-7700HQ @2.80Ghz                  |     16 GB  |   1 TB SSD         |  12 min 54 sec              |\n",
    "| Machine 3 |                   |       |            |                |\n",
    "| Machine 4 |                   |       |            |                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we attempt to extract the data dump we downloaded `data.zip` into individual uncompressed csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with zipfile.ZipFile(os.path.join(output_directory, \"data.zip\"), 'r') as f:\n",
    "    f.extractall(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of the extraction operation across 4 different machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|           | CPU               | RAM   | HD         | Operation time |\n",
    "|-----------|-------------------|-------|------------|----------------|\n",
    "| Machine 1 | i5-4460 @ 3.20Ghz | 10 GB | 1 TB SSD | 19.5 sec   |\n",
    "| Machine 2 | i7-7700HQ @ 2.80Ghz                  |     16 GB  |   1 TB SSD         |  20.3 sec              |\n",
    "| Machine 3 |                   |       |            |                |\n",
    "| Machine 4 |                   |       |            |                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combining data CSVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we attempt to combine the individuals csv files into a single pandas dataframe which we then save to a single csv file called `combined_data.csv`. We create a new column called `model` to be able to identify which dataset each record originally comes from. The names of the models are extracted using regex from the csv file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 3476.37 MiB, increment: 0.02 MiB\n",
      "Wall time: 7min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit\n",
    "files = glob.glob('rainfall/*.csv')\n",
    "df = pd.concat((pd.read_csv(file, index_col=0)\n",
    "                .assign(model=re.findall(r'[^\\/]+(?=\\_d)', file)[0]) # use r'[^\\\\]+(?=\\_d)' on Windows machines\n",
    "                for file in files)\n",
    "              )\n",
    "df.to_csv(\"rainfall/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of the concatenation operation across 4 different machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|           | CPU               | RAM   | HD         | Operation time |\n",
    "|-----------|-------------------|-------|------------|----------------|\n",
    "| Machine 1 | i5-4460 @ 3.20Ghz | 10 GB | 1 TB SSD | 5 min 34 sec  |\n",
    "| Machine 2 | i7-7700HQ @ 2.80Ghz                  |     16 GB  |   1 TB SSD         |  7 min 38 sec              |\n",
    "| Machine 3 |                   |       |            |                |\n",
    "| Machine 4 |                   |       |            |                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load the combined CSV to memory and perform a simple EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the size of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.6G\trainfall/combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "du -sh rainfall/combined_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the combined dataset is 5.6 GB! We attempt to load this entire file into memory using pandas in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"rainfall/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62513863, 7)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>rain (mm/day)</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1889-01-01 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>3.293256e-13</td>\n",
       "      <td>rainfall\\ACCESS-CM2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1889-01-02 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>rainfall\\ACCESS-CM2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1889-01-03 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>rainfall\\ACCESS-CM2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1889-01-04 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>rainfall\\ACCESS-CM2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1889-01-05 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>1.047658e-02</td>\n",
       "      <td>rainfall\\ACCESS-CM2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time  lat_min  lat_max  lon_min  lon_max  rain (mm/day)  \\\n",
       "0  1889-01-01 12:00:00   -36.25    -35.0  140.625    142.5   3.293256e-13   \n",
       "1  1889-01-02 12:00:00   -36.25    -35.0  140.625    142.5   0.000000e+00   \n",
       "2  1889-01-03 12:00:00   -36.25    -35.0  140.625    142.5   0.000000e+00   \n",
       "3  1889-01-04 12:00:00   -36.25    -35.0  140.625    142.5   0.000000e+00   \n",
       "4  1889-01-05 12:00:00   -36.25    -35.0  140.625    142.5   1.047658e-02   \n",
       "\n",
       "                 model  \n",
       "0  rainfall\\ACCESS-CM2  \n",
       "1  rainfall\\ACCESS-CM2  \n",
       "2  rainfall\\ACCESS-CM2  \n",
       "3  rainfall\\ACCESS-CM2  \n",
       "4  rainfall\\ACCESS-CM2  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: Attempting to read the above csv file results in a dead kernel in JupyterLab on some of our machines. System Resources Monitor shows a steady increase in RAM usage until 100% after which the jupyterlab notebook crashes as seen in the image below.\n",
    "\n",
    "![out-of-memory](img/machine1-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of the loading operation across 4 different machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|           | CPU               | RAM   | HD         | Operation time |\n",
    "|-----------|-------------------|-------|------------|----------------|\n",
    "| Machine 1 | i5-4460 @ 3.20Ghz | 10 GB | 1 TB SSD | OUT OF MEMORY ERROR  |\n",
    "| Machine 2 | i7-7700HQ @ 2.80Ghz                  |     16 GB  |   1 TB SSD         |  1min 27s              |\n",
    "| Machine 3 |                   |       |            |                |\n",
    "| Machine 4 |                   |       |            |                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DASK approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the limitations faced when loading the dataset using pandas, we attempt to use DASK - a python library that allows for parallel computing and works better with large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 5018.77 MiB, increment: 4856.81 MiB\n",
      "CPU times: user 6min 37s, sys: 14.4 s, total: 6min 51s\n",
      "Wall time: 6min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# shows time that dask take to merge\n",
    "ddf = dd.read_csv(\"rainfall/combined_data.csv\",assume_missing=True, dtype={'lon_min': 'object'})\n",
    "ddf.to_csv(\"rainfall/combined_data_dask.csv\", single_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI-ESM1-2-HR       5154240\n",
      "TaiESM1             3541230\n",
      "NorESM2-MM          3541230\n",
      "CMCC-CM2-HR4        3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-ESM2           3541230\n",
      "SAM0-UNICON         3541153\n",
      "FGOALS-f3-L         3219300\n",
      "GFDL-CM4            3219300\n",
      "GFDL-ESM4           3219300\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "MRI-ESM2-0          3037320\n",
      "BCC-CSM2-MR         3035340\n",
      "MIROC6              2070900\n",
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "INM-CM5-0           1609650\n",
      "INM-CM4-8           1609650\n",
      "KIOST-ESM           1287720\n",
      "FGOALS-g3           1287720\n",
      "MPI-ESM1-2-LR        966420\n",
      "NESM3                966420\n",
      "AWI-ESM-1-1-LR       966420\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "NorESM2-LM           919800\n",
      "BCC-ESM1             551880\n",
      "CanESM5              551880\n",
      "Name: model, dtype: int64\n",
      "peak memory: 6295.63 MiB, increment: 1556.75 MiB\n",
      "Wall time: 44.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "## count the number of records for each model\n",
    "print(ddf[\"model\"].value_counts().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               rain (mm/day)          \n",
      "                        mean       std\n",
      "model                                 \n",
      "ACCESS-CM2          1.787025  5.914188\n",
      "ACCESS-ESM1-5       2.217501  6.422397\n",
      "AWI-ESM-1-1-LR      2.026071  5.321889\n",
      "BCC-CSM2-MR         1.951832  6.200969\n",
      "BCC-ESM1            1.811032  5.358361\n",
      "peak memory: 6305.39 MiB, increment: 1577.31 MiB\n",
      "Wall time: 42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "## calculate the mean and std of rain amount grouping by model\n",
    "print(ddf.groupby('model').agg({'rain (mm/day)': ['mean', 'std']}).compute().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in separate chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also attempted to use pandas's chunksize argument to limit the number of lines that are read into local memory at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "import numpy as np\n",
    "rain_total = 0\n",
    "num_entries = 0\n",
    "for chunk in pd.read_csv(\"rainfall/combined_data.csv\", chunksize=10_000_000):\n",
    "    num_entries = num_entries + chunk.shape[0]\n",
    "    rain_total = rain_total + np.sum(chunk['rain (mm/day)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_total / num_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using chunksize argument of read_csv we are able to work-around the memory limitation we previously experienced. Here, for example, we were able to calculate the average rain fall per day across all days included in the data dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Perform a simple EDA in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer dataframe to R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to transfer the dataframe to R we will attempt to use `parquet` - a protable language-agnostic file format for storing dataframes from the Apache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will use the `datasets` module from the `pyarrow` library to create a memory-efficient representation of the combined `csv` file. Arrow represetnation are columnar and memory efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "dataset = ds.dataset(\"rainfall/combined_data.csv\", format=\"csv\")\n",
    "arrow_table = dataset.to_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can write the `Arrow` table to a `parquet` file on disk as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "pq.write_table(arrow_table, 'rainfall/combined_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen below, the size of the resulting `parquet` file is much smaller compared to the combined `csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "542M\trainfall/combined_data.parquet\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "du -sh rainfall/combined_data.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In R, we can utility the `read_parquet` function from `arrow` package to load the file into memory and create an R dataframe. From there, we perform some basic EDA by counting the records we have for each model in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: \n",
      "Attaching package: ‘arrow’\n",
      "\n",
      "\n",
      "R[write to console]: The following object is masked from ‘package:utils’:\n",
      "\n",
      "    timestamp\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: \n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m# A tibble: 28 x 2\u001b[39m\n",
      "   model                  n\n",
      "   \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m              \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m\n",
      "\u001b[90m 1\u001b[39m ACCESS-CM2       1\u001b[4m9\u001b[24m\u001b[4m3\u001b[24m\u001b[4m2\u001b[24m840\n",
      "\u001b[90m 2\u001b[39m ACCESS-ESM1-5    1\u001b[4m6\u001b[24m\u001b[4m1\u001b[24m\u001b[4m0\u001b[24m700\n",
      "\u001b[90m 3\u001b[39m AWI-ESM-1-1-LR    \u001b[4m9\u001b[24m\u001b[4m6\u001b[24m\u001b[4m6\u001b[24m420\n",
      "\u001b[90m 4\u001b[39m BCC-CSM2-MR      3\u001b[4m0\u001b[24m\u001b[4m3\u001b[24m\u001b[4m5\u001b[24m340\n",
      "\u001b[90m 5\u001b[39m BCC-ESM1          \u001b[4m5\u001b[24m\u001b[4m5\u001b[24m\u001b[4m1\u001b[24m880\n",
      "\u001b[90m 6\u001b[39m CanESM5           \u001b[4m5\u001b[24m\u001b[4m5\u001b[24m\u001b[4m1\u001b[24m880\n",
      "\u001b[90m 7\u001b[39m CMCC-CM2-HR4     3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m 8\u001b[39m CMCC-CM2-SR5     3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m 9\u001b[39m CMCC-ESM2        3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m10\u001b[39m EC-Earth3-Veg-LR 3\u001b[4m0\u001b[24m\u001b[4m3\u001b[24m\u001b[4m7\u001b[24m320\n",
      "\u001b[90m# … with 18 more rows\u001b[39m\n",
      "Time difference of 7.321429 secs\n",
      "CPU times: user 8.69 s, sys: 3.45 s, total: 12.1 s\n",
      "Wall time: 7.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "library(arrow)\n",
    "start_time <- Sys.time()\n",
    "r_data <- read_parquet(\"rainfall/combined_data.parquet\")\n",
    "print(class(r_data))\n",
    "library(dplyr)\n",
    "result <- r_data %>% \n",
    "    count(model)\n",
    "end_time <- Sys.time()\n",
    "print(result)\n",
    "print(end_time - start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rationale for choosing this approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use `parquet` file as the mode of exchange between Python and R. `parquet` was choosen for a number of reasons. First, `parquet` files on disk take up less space when compared to conventional file formats such as `csv`. Even compared to the Arrow-based file format `feather`, `parquet` file was approxiamtely 50% smaller than `feather` file for this dataset. (542MB vs 1.1GB). The other reason `parquet` was chosen is performance. Loading and reading of `parquet` files into memory is efficient in both languages, Python and R. As seen in our R test above, loading the dataset into memory and performing some basic EDA took approximately 7.75 seconds. This result can be contrasted with loading times observed when loading the same dataset in `csv` format in Python (fastest time was greater than 60 seconds). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:525]",
   "language": "python",
   "name": "conda-env-525-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

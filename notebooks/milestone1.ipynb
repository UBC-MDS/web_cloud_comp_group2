{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose:\n",
    "In this notebook, we will attempt to download a data dump containing daily rainfall over NSW, Australia dataset found on [figshare](https://figshare.com/articles/dataset/Daily_rainfall_over_NSW_Australia/14096681). This data dump is comprised of different files contained in a 776.4 MB compressed format. This size of the uncompressed data dump is approximately 6.6 GB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import pandas as pd\n",
    "from memory_profiler import memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the API request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [requests library](https://docs.python-requests.org/en/master/) to generate an http request call to the [figshare API](https://docs.figshare.com/). Specifically, we will make a call to the `/articles` endpoint to retrieve information about the article of interest including the url we need to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id = 14096681\n",
    "base_url = 'https://api.figshare.com/v2'\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "endpoint = f'/articles/{article_id}'\n",
    "output_directory = \"rainfall/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'defined_type_name': 'dataset',\n",
       " 'embargo_date': None,\n",
       " 'citation': 'Beuzen, Tomas (2021): Daily rainfall over NSW, Australia. figshare. Dataset. https://doi.org/10.6084/m9.figshare.14096681.v3',\n",
       " 'url_private_api': 'https://api.figshare.com/v2/account/articles/14096681',\n",
       " 'embargo_reason': '',\n",
       " 'references': ['https://www.wcrp-climate.org/wgcm-cmip/wgcm-cmip6',\n",
       "  'https://pangeo-data.github.io/pangeo-cmip6-cloud/',\n",
       "  'https://www.longpaddock.qld.gov.au/silo/'],\n",
       " 'funding_list': [],\n",
       " 'url_public_api': 'https://api.figshare.com/v2/articles/14096681',\n",
       " 'id': 14096681,\n",
       " 'custom_fields': [],\n",
       " 'size': 814109773,\n",
       " 'metadata_reason': '',\n",
       " 'funding': None,\n",
       " 'figshare_url': 'https://figshare.com/articles/dataset/Daily_rainfall_over_NSW_Australia/14096681',\n",
       " 'embargo_type': 'file',\n",
       " 'title': 'Daily rainfall over NSW, Australia',\n",
       " 'defined_type': 3,\n",
       " 'embargo_options': [],\n",
       " 'is_embargoed': False,\n",
       " 'version': 3,\n",
       " 'resource_doi': None,\n",
       " 'url_public_html': 'https://figshare.com/articles/dataset/Daily_rainfall_over_NSW_Australia/14096681',\n",
       " 'confidential_reason': '',\n",
       " 'files': [{'is_link_only': False,\n",
       "   'name': 'daily_rainfall_2014.png',\n",
       "   'supplied_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "   'computed_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "   'id': 26579150,\n",
       "   'download_url': 'https://ndownloader.figshare.com/files/26579150',\n",
       "   'size': 58863},\n",
       "  {'is_link_only': False,\n",
       "   'name': 'environment.yml',\n",
       "   'supplied_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "   'computed_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "   'id': 26579171,\n",
       "   'download_url': 'https://ndownloader.figshare.com/files/26579171',\n",
       "   'size': 192},\n",
       "  {'is_link_only': False,\n",
       "   'name': 'README.md',\n",
       "   'supplied_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "   'computed_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "   'id': 26586554,\n",
       "   'download_url': 'https://ndownloader.figshare.com/files/26586554',\n",
       "   'size': 5422},\n",
       "  {'is_link_only': False,\n",
       "   'name': 'data.zip',\n",
       "   'supplied_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "   'computed_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "   'id': 26766812,\n",
       "   'download_url': 'https://ndownloader.figshare.com/files/26766812',\n",
       "   'size': 814041183},\n",
       "  {'is_link_only': False,\n",
       "   'name': 'get_data.py',\n",
       "   'supplied_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "   'computed_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "   'id': 26766815,\n",
       "   'download_url': 'https://ndownloader.figshare.com/files/26766815',\n",
       "   'size': 4113}],\n",
       " 'handle': '',\n",
       " 'description': '<div><div><div>This data dump contains modelled and observed daily rainfall data over NSW, Australia, spanning 1889 - 2014. Please see README.md for more information.</div></div></div>',\n",
       " 'tags': ['rainfall data', 'Python'],\n",
       " 'timeline': {'revision': '2021-03-29T20:56:29',\n",
       "  'firstOnline': '2021-02-23T16:57:52',\n",
       "  'posted': '2021-03-29T20:56:29'},\n",
       " 'url_private_html': 'https://figshare.com/account/articles/14096681',\n",
       " 'published_date': '2021-03-29T20:56:29Z',\n",
       " 'modified_date': '2021-03-29T20:56:31Z',\n",
       " 'authors': [{'url_name': 'Tomas_Beuzen',\n",
       "   'is_active': True,\n",
       "   'id': 10182999,\n",
       "   'full_name': 'Tomas Beuzen',\n",
       "   'orcid_id': ''}],\n",
       " 'is_public': True,\n",
       " 'categories': [{'parent_id': 33, 'id': 78, 'title': 'Atmospheric Sciences'},\n",
       "  {'parent_id': 33, 'id': 204, 'title': 'Climate Change Processes'},\n",
       "  {'parent_id': 33, 'id': 144, 'title': 'Climate Science'},\n",
       "  {'parent_id': 33,\n",
       "   'id': 205,\n",
       "   'title': 'Climatology (excl. Climate Change Processes)'},\n",
       "  {'parent_id': 33, 'id': 260, 'title': 'Environmental Management'},\n",
       "  {'parent_id': 33, 'id': 34, 'title': 'Environmental Science'},\n",
       "  {'parent_id': 33, 'id': 80, 'title': 'Hydrology'},\n",
       "  {'parent_id': 33, 'id': 207, 'title': 'Meteorology'}],\n",
       " 'thumb': 'https://s3-eu-west-1.amazonaws.com/pfigshare-u-previews/26579150/thumb.png',\n",
       " 'is_confidential': False,\n",
       " 'doi': '10.6084/m9.figshare.14096681.v3',\n",
       " 'has_linked_file': False,\n",
       " 'license': {'url': 'https://creativecommons.org/licenses/by/4.0/',\n",
       "  'name': 'CC BY 4.0',\n",
       "  'value': 1},\n",
       " 'url': 'https://api.figshare.com/v2/articles/14096681',\n",
       " 'resource_title': None,\n",
       " 'status': 'public',\n",
       " 'created_date': '2021-03-29T20:56:29Z',\n",
       " 'group_id': None,\n",
       " 'is_metadata_record': False}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.request(\"GET\", base_url+endpoint, headers=headers)\n",
    "data = json.loads(response.text)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response above contains a `data` json key which is of interest of us. It lists the files corresponding to the article along with their download urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'is_link_only': False,\n",
       "  'name': 'daily_rainfall_2014.png',\n",
       "  'supplied_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "  'computed_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "  'id': 26579150,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579150',\n",
       "  'size': 58863},\n",
       " {'is_link_only': False,\n",
       "  'name': 'environment.yml',\n",
       "  'supplied_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "  'computed_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "  'id': 26579171,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579171',\n",
       "  'size': 192},\n",
       " {'is_link_only': False,\n",
       "  'name': 'README.md',\n",
       "  'supplied_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "  'computed_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "  'id': 26586554,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26586554',\n",
       "  'size': 5422},\n",
       " {'is_link_only': False,\n",
       "  'name': 'data.zip',\n",
       "  'supplied_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "  'computed_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "  'id': 26766812,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766812',\n",
       "  'size': 814041183},\n",
       " {'is_link_only': False,\n",
       "  'name': 'get_data.py',\n",
       "  'supplied_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "  'computed_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "  'id': 26766815,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766815',\n",
       "  'size': 4113}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = data[\"files\"]           \n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data dump is `data.zip` which can be retreived as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.11 s, sys: 4.54 s, total: 10.7 s\n",
      "Wall time: 2min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "files_to_dl = [\"data.zip\"] \n",
    "for file in files:\n",
    "    if file[\"name\"] in files_to_dl:\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        urlretrieve(file[\"download_url\"], output_directory + file[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the output above, the single act of downloading the data dump to the local machine took around 2mins 9s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of running time for the download operation across 4 different machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|           | CPU               | RAM   | HD         | Operation time |\n",
    "|-----------|-------------------|-------|------------|----------------|\n",
    "| Machine 1 | i5-4460 @ 3.20Ghz | 10 GB | 1 TB SSD | 2 min 9 sec   |\n",
    "| Machine 2 |                   |       |            |                |\n",
    "| Machine 3 |                   |       |            |                |\n",
    "| Machine 4 |                   |       |            |                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we attempt to extract the data dump we downloaded `data.zip` into individual uncompressed csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.7 s, sys: 2.28 s, total: 19 s\n",
      "Wall time: 19.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with zipfile.ZipFile(os.path.join(output_directory, \"data.zip\"), 'r') as f:\n",
    "    f.extractall(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of running time for the extraction operation across 4 different machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|           | CPU               | RAM   | HD         | Operation time |\n",
    "|-----------|-------------------|-------|------------|----------------|\n",
    "| Machine 1 | i5-4460 @ 3.20Ghz | 10 GB | 1 TB SSD | 19.5 sec   |\n",
    "| Machine 2 |                   |       |            |                |\n",
    "| Machine 3 |                   |       |            |                |\n",
    "| Machine 4 |                   |       |            |                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we attempt to combine the individuals csv files into a single pandas dataframe which we then save to a single csv file called `combined_data.csv`. We create a new column called `model` to be able to identify which dataset each record originally comes from. The names of the models are extracted using regex from the csv file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 183.35 MiB, increment: 0.30 MiB\n",
      "CPU times: user 5min 43s, sys: 12 s, total: 5min 55s\n",
      "Wall time: 5min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit\n",
    "files = glob.glob('rainfall/*.csv')\n",
    "df = pd.concat((pd.read_csv(file, index_col=0)\n",
    "                .assign(model=re.findall(r'[^\\\\]+(?=\\_d)', file)[0])\n",
    "                for file in files)\n",
    "              )\n",
    "df.to_csv(\"rainfall/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.1G\trainfall/combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "du -sh rainfall/combined_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the combined dataset is 6.1 GB!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"rainfall/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempting to read the above csv file results in a dead kernel in jupyterlab. System Resources Monitor shows a steady increase in RAM usage until 100% after which the jupyterlab notebook crashes as seen in the image below.\n",
    "![out-of-memory](img/machine1-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of the concatenation operation across 4 different machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|           | CPU               | RAM   | HD         | Operation time |\n",
    "|-----------|-------------------|-------|------------|----------------|\n",
    "| Machine 1 | i5-4460 @ 3.20Ghz | 10 GB | 1 TB SSD | OUT OF MEMORY ERROR  |\n",
    "| Machine 2 |                   |       |            |                |\n",
    "| Machine 3 |                   |       |            |                |\n",
    "| Machine 4 |                   |       |            |                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DASK Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:525]",
   "language": "python",
   "name": "conda-env-525-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

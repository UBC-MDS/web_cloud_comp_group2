{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose:\n",
    "In this notebook, we will attempt to download a data dump containing daily rainfall over NSW, Australia dataset found on [figshare](https://figshare.com/articles/dataset/Daily_rainfall_over_NSW_Australia/14096681). This data dump is comprised of different files contained in a 776.4 MB compressed format. This size of the uncompressed data dump is approximately 6.6 GB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import pandas as pd\n",
    "from memory_profiler import memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n",
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext rpy2.ipython\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the API request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [requests library](https://docs.python-requests.org/en/master/) to generate an http request call to the [figshare API](https://docs.figshare.com/). Specifically, we will make a call to the `/articles` endpoint to retrieve information about the article of interest including the url we need to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id = 14096681\n",
    "base_url = 'https://api.figshare.com/v2'\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "endpoint = f'/articles/{article_id}'\n",
    "output_directory = \"rainfall/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'defined_type_name': 'dataset',\n",
       " 'embargo_date': None,\n",
       " 'citation': 'Beuzen, Tomas (2021): Daily rainfall over NSW, Australia. figshare. Dataset. https://doi.org/10.6084/m9.figshare.14096681.v3',\n",
       " 'url_private_api': 'https://api.figshare.com/v2/account/articles/14096681',\n",
       " 'embargo_reason': '',\n",
       " 'references': ['https://www.wcrp-climate.org/wgcm-cmip/wgcm-cmip6',\n",
       "  'https://pangeo-data.github.io/pangeo-cmip6-cloud/',\n",
       "  'https://www.longpaddock.qld.gov.au/silo/'],\n",
       " 'funding_list': [],\n",
       " 'url_public_api': 'https://api.figshare.com/v2/articles/14096681',\n",
       " 'id': 14096681,\n",
       " 'custom_fields': [],\n",
       " 'size': 814109773,\n",
       " 'metadata_reason': '',\n",
       " 'funding': None,\n",
       " 'figshare_url': 'https://figshare.com/articles/dataset/Daily_rainfall_over_NSW_Australia/14096681',\n",
       " 'embargo_type': 'file',\n",
       " 'title': 'Daily rainfall over NSW, Australia',\n",
       " 'defined_type': 3,\n",
       " 'embargo_options': [],\n",
       " 'is_embargoed': False,\n",
       " 'version': 3,\n",
       " 'resource_doi': None,\n",
       " 'url_public_html': 'https://figshare.com/articles/dataset/Daily_rainfall_over_NSW_Australia/14096681',\n",
       " 'confidential_reason': '',\n",
       " 'files': [{'is_link_only': False,\n",
       "   'name': 'daily_rainfall_2014.png',\n",
       "   'supplied_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "   'computed_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "   'id': 26579150,\n",
       "   'download_url': 'https://ndownloader.figshare.com/files/26579150',\n",
       "   'size': 58863},\n",
       "  {'is_link_only': False,\n",
       "   'name': 'environment.yml',\n",
       "   'supplied_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "   'computed_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "   'id': 26579171,\n",
       "   'download_url': 'https://ndownloader.figshare.com/files/26579171',\n",
       "   'size': 192},\n",
       "  {'is_link_only': False,\n",
       "   'name': 'README.md',\n",
       "   'supplied_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "   'computed_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "   'id': 26586554,\n",
       "   'download_url': 'https://ndownloader.figshare.com/files/26586554',\n",
       "   'size': 5422},\n",
       "  {'is_link_only': False,\n",
       "   'name': 'data.zip',\n",
       "   'supplied_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "   'computed_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "   'id': 26766812,\n",
       "   'download_url': 'https://ndownloader.figshare.com/files/26766812',\n",
       "   'size': 814041183},\n",
       "  {'is_link_only': False,\n",
       "   'name': 'get_data.py',\n",
       "   'supplied_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "   'computed_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "   'id': 26766815,\n",
       "   'download_url': 'https://ndownloader.figshare.com/files/26766815',\n",
       "   'size': 4113}],\n",
       " 'handle': '',\n",
       " 'description': '<div><div><div>This data dump contains modelled and observed daily rainfall data over NSW, Australia, spanning 1889 - 2014. Please see README.md for more information.</div></div></div>',\n",
       " 'tags': ['rainfall data', 'Python'],\n",
       " 'timeline': {'revision': '2021-03-29T20:56:29',\n",
       "  'firstOnline': '2021-02-23T16:57:52',\n",
       "  'posted': '2021-03-29T20:56:29'},\n",
       " 'url_private_html': 'https://figshare.com/account/articles/14096681',\n",
       " 'published_date': '2021-03-29T20:56:29Z',\n",
       " 'modified_date': '2021-03-29T20:56:31Z',\n",
       " 'authors': [{'url_name': 'Tomas_Beuzen',\n",
       "   'is_active': True,\n",
       "   'id': 10182999,\n",
       "   'full_name': 'Tomas Beuzen',\n",
       "   'orcid_id': ''}],\n",
       " 'is_public': True,\n",
       " 'categories': [{'parent_id': 33, 'id': 78, 'title': 'Atmospheric Sciences'},\n",
       "  {'parent_id': 33, 'id': 204, 'title': 'Climate Change Processes'},\n",
       "  {'parent_id': 33, 'id': 144, 'title': 'Climate Science'},\n",
       "  {'parent_id': 33,\n",
       "   'id': 205,\n",
       "   'title': 'Climatology (excl. Climate Change Processes)'},\n",
       "  {'parent_id': 33, 'id': 260, 'title': 'Environmental Management'},\n",
       "  {'parent_id': 33, 'id': 34, 'title': 'Environmental Science'},\n",
       "  {'parent_id': 33, 'id': 80, 'title': 'Hydrology'},\n",
       "  {'parent_id': 33, 'id': 207, 'title': 'Meteorology'}],\n",
       " 'thumb': 'https://s3-eu-west-1.amazonaws.com/pfigshare-u-previews/26579150/thumb.png',\n",
       " 'is_confidential': False,\n",
       " 'doi': '10.6084/m9.figshare.14096681.v3',\n",
       " 'has_linked_file': False,\n",
       " 'license': {'url': 'https://creativecommons.org/licenses/by/4.0/',\n",
       "  'name': 'CC BY 4.0',\n",
       "  'value': 1},\n",
       " 'url': 'https://api.figshare.com/v2/articles/14096681',\n",
       " 'resource_title': None,\n",
       " 'status': 'public',\n",
       " 'created_date': '2021-03-29T20:56:29Z',\n",
       " 'group_id': None,\n",
       " 'is_metadata_record': False}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.request(\"GET\", base_url+endpoint, headers=headers)\n",
    "data = json.loads(response.text)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response above contains a `data` json key which is of interest of us. It lists the files corresponding to the article along with their download urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'is_link_only': False,\n",
       "  'name': 'daily_rainfall_2014.png',\n",
       "  'supplied_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "  'computed_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "  'id': 26579150,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579150',\n",
       "  'size': 58863},\n",
       " {'is_link_only': False,\n",
       "  'name': 'environment.yml',\n",
       "  'supplied_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "  'computed_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "  'id': 26579171,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579171',\n",
       "  'size': 192},\n",
       " {'is_link_only': False,\n",
       "  'name': 'README.md',\n",
       "  'supplied_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "  'computed_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "  'id': 26586554,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26586554',\n",
       "  'size': 5422},\n",
       " {'is_link_only': False,\n",
       "  'name': 'data.zip',\n",
       "  'supplied_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "  'computed_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "  'id': 26766812,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766812',\n",
       "  'size': 814041183},\n",
       " {'is_link_only': False,\n",
       "  'name': 'get_data.py',\n",
       "  'supplied_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "  'computed_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "  'id': 26766815,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766815',\n",
       "  'size': 4113}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = data[\"files\"]           \n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data dump is `data.zip` which can be retreived as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the file of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "files_to_dl = [\"data.zip\"] \n",
    "for file in files:\n",
    "    if file[\"name\"] in files_to_dl:\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        urlretrieve(file[\"download_url\"], output_directory + file[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the output above, the single act of downloading the data dump to the local machine took around 2mins 9s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of the download operation across 4 different machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|           | CPU               | RAM   | HD         | Operation time |\n",
    "|-----------|-------------------|-------|------------|----------------|\n",
    "| Machine 1 | i5-4460 @ 3.20Ghz | 10 GB | 1 TB SSD   |  2 min 9 sec   |\n",
    "| Machine 2 | i7-7700HQ @2.80Ghz| 16 GB | 1 TB SSD   |  12 min 54 sec |\n",
    "| Machine 3 |  i5 @1.60Ghz      | 4 GB  | 121 GB SSD |  5 min 9 sec   |\n",
    "| Machine 4 |                   |       |            |                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we attempt to extract the data dump we downloaded `data.zip` into individual uncompressed csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with zipfile.ZipFile(os.path.join(output_directory, \"data.zip\"), 'r') as f:\n",
    "    f.extractall(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of the extraction operation across 4 different machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|           | CPU               | RAM   | HD         | Operation time |\n",
    "|-----------|-------------------|-------|------------|----------------|\n",
    "| Machine 1 | i5-4460 @ 3.20Ghz | 10 GB | 1 TB SSD | 19.5 sec   |\n",
    "| Machine 2 | i7-7700HQ @ 2.80Ghz                  |     16 GB  |   1 TB SSD         |  20.3 sec              |\n",
    "| Machine 3 | i5 @1.60Ghz      | 4 GB  | 121 GB SSD |  1 min 53 sec   |\n",
    "| Machine 4 |                   |       |            |                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combining data CSVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we attempt to combine the individuals csv files into a single pandas dataframe which we then save to a single csv file called `combined_data.csv`. We create a new column called `model` to be able to identify which dataset each record originally comes from. The names of the models are extracted using regex from the csv file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 3476.37 MiB, increment: 0.02 MiB\n",
      "Wall time: 7min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit\n",
    "files = glob.glob('rainfall/*.csv')\n",
    "df = pd.concat((pd.read_csv(file, index_col=0)\n",
    "                .assign(model=re.findall(r'[^\\/]+(?=\\_d)', file)[0]) # use r'[^\\\\]+(?=\\_d)' on Windows machines\n",
    "                for file in files)\n",
    "              )\n",
    "df.to_csv(\"rainfall/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of the concatenation operation across 4 different machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|           | CPU               | RAM   | HD         | Operation time |\n",
    "|-----------|-------------------|-------|------------|----------------|\n",
    "| Machine 1 | i5-4460 @ 3.20Ghz | 10 GB | 1 TB SSD | 5 min 34 sec  |\n",
    "| Machine 2 | i7-7700HQ @ 2.80Ghz                  |     16 GB  |   1 TB SSD         |  7 min 38 sec              |\n",
    "| Machine 3 | i5 @1.60Ghz      | 4 GB  | 121 GB SSD |  Out of memory error   |\n",
    "| Machine 4 |                   |       |            |                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load the combined CSV to memory and perform a simple EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the size of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.2G\trainfall/combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "du -sh rainfall/combined_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the combined dataset is 5.6 GB! We attempt to load this entire file into memory using pandas in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"rainfall/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62513863, 7)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>rain (mm/day)</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1889-01-01 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>3.293256e-13</td>\n",
       "      <td>rainfall\\ACCESS-CM2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1889-01-02 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>rainfall\\ACCESS-CM2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1889-01-03 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>rainfall\\ACCESS-CM2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1889-01-04 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>rainfall\\ACCESS-CM2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1889-01-05 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>1.047658e-02</td>\n",
       "      <td>rainfall\\ACCESS-CM2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time  lat_min  lat_max  lon_min  lon_max  rain (mm/day)  \\\n",
       "0  1889-01-01 12:00:00   -36.25    -35.0  140.625    142.5   3.293256e-13   \n",
       "1  1889-01-02 12:00:00   -36.25    -35.0  140.625    142.5   0.000000e+00   \n",
       "2  1889-01-03 12:00:00   -36.25    -35.0  140.625    142.5   0.000000e+00   \n",
       "3  1889-01-04 12:00:00   -36.25    -35.0  140.625    142.5   0.000000e+00   \n",
       "4  1889-01-05 12:00:00   -36.25    -35.0  140.625    142.5   1.047658e-02   \n",
       "\n",
       "                 model  \n",
       "0  rainfall\\ACCESS-CM2  \n",
       "1  rainfall\\ACCESS-CM2  \n",
       "2  rainfall\\ACCESS-CM2  \n",
       "3  rainfall\\ACCESS-CM2  \n",
       "4  rainfall\\ACCESS-CM2  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempting to read the above csv file results in a dead kernel in JupyterLab. System Resources Monitor shows a steady increase in RAM usage until 100% after which the jupyterlab notebook crashes as seen in the image below.\n",
    "![out-of-memory](img/machine1-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of the loading operation across 4 different machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|           | CPU               | RAM   | HD         | Operation time |\n",
    "|-----------|-------------------|-------|------------|----------------|\n",
    "| Machine 1 | i5-4460 @ 3.20Ghz | 10 GB | 1 TB SSD | OUT OF MEMORY ERROR  |\n",
    "| Machine 2 | i7-7700HQ @ 2.80Ghz                  |     16 GB  |   1 TB SSD         |  1min 27s              |\n",
    "| Machine 3 | i5 @1.60Ghz      | 4 GB  | 121 GB SSD |  Out of memory error   |\n",
    "| Machine 4 |                   |       |            |                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DASK approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the limitations faced when loading the dataset using pandas, we attempt to use DASK - a python library that allows for parallel computing and works better with large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 11087.65 MiB, increment: 5908.88 MiB\n",
      "Wall time: 8min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# shows time that dask take to merge\n",
    "ddf = dd.read_csv(\"data/combined_data.csv\",assume_missing=True, dtype={'lon_min': 'object'})\n",
    "ddf.to_csv(\"data/combined_data_dask.csv\", single_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI-ESM1-2-HR       5154240\n",
      "TaiESM1             3541230\n",
      "NorESM2-MM          3541230\n",
      "CMCC-CM2-HR4        3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-ESM2           3541230\n",
      "SAM0-UNICON         3541153\n",
      "FGOALS-f3-L         3219300\n",
      "GFDL-CM4            3219300\n",
      "GFDL-ESM4           3219300\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "MRI-ESM2-0          3037320\n",
      "BCC-CSM2-MR         3035340\n",
      "MIROC6              2070900\n",
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "INM-CM5-0           1609650\n",
      "INM-CM4-8           1609650\n",
      "KIOST-ESM           1287720\n",
      "FGOALS-g3           1287720\n",
      "MPI-ESM1-2-LR        966420\n",
      "NESM3                966420\n",
      "AWI-ESM-1-1-LR       966420\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "NorESM2-LM           919800\n",
      "BCC-ESM1             551880\n",
      "CanESM5              551880\n",
      "Name: model, dtype: int64\n",
      "peak memory: 6295.63 MiB, increment: 1556.75 MiB\n",
      "Wall time: 44.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "## count the number of records for each model\n",
    "print(ddf[\"model\"].value_counts().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               rain (mm/day)          \n",
      "                        mean       std\n",
      "model                                 \n",
      "ACCESS-CM2          1.787025  5.914188\n",
      "ACCESS-ESM1-5       2.217501  6.422397\n",
      "AWI-ESM-1-1-LR      2.026071  5.321889\n",
      "BCC-CSM2-MR         1.951832  6.200969\n",
      "BCC-ESM1            1.811032  5.358361\n",
      "peak memory: 6305.39 MiB, increment: 1577.31 MiB\n",
      "Wall time: 42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "## calculate the mean and std of rain amount grouping by model\n",
    "print(ddf.groupby('model').agg({'rain (mm/day)': ['mean', 'std']}).compute().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in separate chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also attempted to use pandas's chunksize argument to limit the number of lines that are read into local memory at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "import numpy as np\n",
    "rain_total = 0\n",
    "num_entries = 0\n",
    "for chunk in pd.read_csv(\"rainfall/combined_data.csv\", chunksize=10_000_000):\n",
    "    num_entries = num_entries + chunk.shape[0]\n",
    "    rain_total = rain_total + np.sum(chunk['rain (mm/day)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_total / num_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using chunksize argument of read_csv we are able to work-around the memory limitation we previously experienced. Here, for example, we were able to calculate the average rain fall per day across all days included in the data dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading particular columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also attempted to only load some columns that could be the more important for our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "use_cols = [\"time\", \"lat_min\", \"lat_max\", \"lon_min\", \"lon_max\", \"rain (mm/day)\", \"model\"]\n",
    "df = pd.read_csv(\"rainfall/combined_data.csv\", usecols=use_cols)\n",
    "print(df[\"model\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "use_cols = [\"time\", \"model\"]\n",
    "df = pd.read_csv(\"rainfall/combined_data.csv\", usecols=use_cols)\n",
    "print(df[\"model\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By only loading some columns that are directly related to the process of interest (in this case counting how many data point of each model), we are able to reduce the memory and time requirement for this process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Perform a simple EDA in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer dataframe to R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to transfer the dataframe to R we will attempt to use `feather` - a protable language-agnostic file format for storing dataframes and sharing them between R and python projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.feather as feather\n",
    "#feather.write_feather(df, '/rainfall/combined_data_feather')\n",
    "##\n",
    "##\n",
    "## TO DO\n",
    "##\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:525]",
   "language": "python",
   "name": "conda-env-525-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

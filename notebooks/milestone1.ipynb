{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose:\n",
    "In this notebook, we will attempt to download a data dump containing daily rainfall over NSW, Australia dataset found on [figshare](https://figshare.com/articles/dataset/Daily_rainfall_over_NSW_Australia/14096681). This data dump is comprised of different files contained in a 776.4 MB compressed format. This size of the uncompressed data dump is approximately 6.6 GB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import pandas as pd\n",
    "from memory_profiler import memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n",
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext rpy2.ipython\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the API request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [requests library](https://docs.python-requests.org/en/master/) to generate an http request call to the [figshare API](https://docs.figshare.com/). Specifically, we will make a call to the `/articles` endpoint to retrieve information about the article of interest including the url we need to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id = 14096681\n",
    "base_url = 'https://api.figshare.com/v2'\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "endpoint = f'/articles/{article_id}'\n",
    "output_directory = \"rainfall/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'defined_type_name': 'dataset',\n",
       " 'embargo_date': None,\n",
       " 'citation': 'Beuzen, Tomas (2021): Daily rainfall over NSW, Australia. figshare. Dataset. https://doi.org/10.6084/m9.figshare.14096681.v3',\n",
       " 'url_private_api': 'https://api.figshare.com/v2/account/articles/14096681',\n",
       " 'embargo_reason': '',\n",
       " 'references': ['https://www.wcrp-climate.org/wgcm-cmip/wgcm-cmip6',\n",
       "  'https://pangeo-data.github.io/pangeo-cmip6-cloud/',\n",
       "  'https://www.longpaddock.qld.gov.au/silo/'],\n",
       " 'funding_list': [],\n",
       " 'url_public_api': 'https://api.figshare.com/v2/articles/14096681',\n",
       " 'id': 14096681,\n",
       " 'custom_fields': [],\n",
       " 'size': 814109773,\n",
       " 'metadata_reason': '',\n",
       " 'funding': None,\n",
       " 'figshare_url': 'https://figshare.com/articles/dataset/Daily_rainfall_over_NSW_Australia/14096681',\n",
       " 'embargo_type': 'file',\n",
       " 'title': 'Daily rainfall over NSW, Australia',\n",
       " 'defined_type': 3,\n",
       " 'embargo_options': [],\n",
       " 'is_embargoed': False,\n",
       " 'version': 3,\n",
       " 'embargo_title': '',\n",
       " 'url_public_html': 'https://figshare.com/articles/dataset/Daily_rainfall_over_NSW_Australia/14096681',\n",
       " 'confidential_reason': '',\n",
       " 'resource_doi': None,\n",
       " 'files': [{'is_link_only': False,\n",
       "   'name': 'daily_rainfall_2014.png',\n",
       "   'supplied_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "   'computed_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "   'id': 26579150,\n",
       "   'download_url': 'https://ndownloader.figshare.com/files/26579150',\n",
       "   'size': 58863},\n",
       "  {'is_link_only': False,\n",
       "   'name': 'environment.yml',\n",
       "   'supplied_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "   'computed_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "   'id': 26579171,\n",
       "   'download_url': 'https://ndownloader.figshare.com/files/26579171',\n",
       "   'size': 192},\n",
       "  {'is_link_only': False,\n",
       "   'name': 'README.md',\n",
       "   'supplied_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "   'computed_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "   'id': 26586554,\n",
       "   'download_url': 'https://ndownloader.figshare.com/files/26586554',\n",
       "   'size': 5422},\n",
       "  {'is_link_only': False,\n",
       "   'name': 'data.zip',\n",
       "   'supplied_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "   'computed_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "   'id': 26766812,\n",
       "   'download_url': 'https://ndownloader.figshare.com/files/26766812',\n",
       "   'size': 814041183},\n",
       "  {'is_link_only': False,\n",
       "   'name': 'get_data.py',\n",
       "   'supplied_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "   'computed_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "   'id': 26766815,\n",
       "   'download_url': 'https://ndownloader.figshare.com/files/26766815',\n",
       "   'size': 4113}],\n",
       " 'handle': '',\n",
       " 'description': '<div><div><div>This data dump contains modelled and observed daily rainfall data over NSW, Australia, spanning 1889 - 2014. Please see README.md for more information.</div></div></div>',\n",
       " 'tags': ['rainfall data', 'Python'],\n",
       " 'timeline': {'revision': '2021-03-29T20:56:29',\n",
       "  'firstOnline': '2021-02-23T16:57:52',\n",
       "  'posted': '2021-03-29T20:56:29'},\n",
       " 'url_private_html': 'https://figshare.com/account/articles/14096681',\n",
       " 'published_date': '2021-03-29T20:56:29Z',\n",
       " 'modified_date': '2021-03-29T20:56:31Z',\n",
       " 'authors': [{'url_name': 'Tomas_Beuzen',\n",
       "   'is_active': True,\n",
       "   'id': 10182999,\n",
       "   'full_name': 'Tomas Beuzen',\n",
       "   'orcid_id': ''}],\n",
       " 'is_public': True,\n",
       " 'categories': [{'parent_id': 33, 'id': 78, 'title': 'Atmospheric Sciences'},\n",
       "  {'parent_id': 33, 'id': 204, 'title': 'Climate Change Processes'},\n",
       "  {'parent_id': 33, 'id': 144, 'title': 'Climate Science'},\n",
       "  {'parent_id': 33,\n",
       "   'id': 205,\n",
       "   'title': 'Climatology (excl. Climate Change Processes)'},\n",
       "  {'parent_id': 33, 'id': 260, 'title': 'Environmental Management'},\n",
       "  {'parent_id': 33, 'id': 34, 'title': 'Environmental Science'},\n",
       "  {'parent_id': 33, 'id': 80, 'title': 'Hydrology'},\n",
       "  {'parent_id': 33, 'id': 207, 'title': 'Meteorology'}],\n",
       " 'thumb': 'https://s3-eu-west-1.amazonaws.com/pfigshare-u-previews/26579150/thumb.png',\n",
       " 'is_confidential': False,\n",
       " 'doi': '10.6084/m9.figshare.14096681.v3',\n",
       " 'has_linked_file': False,\n",
       " 'license': {'url': 'https://creativecommons.org/licenses/by/4.0/',\n",
       "  'name': 'CC BY 4.0',\n",
       "  'value': 1},\n",
       " 'url': 'https://api.figshare.com/v2/articles/14096681',\n",
       " 'resource_title': None,\n",
       " 'status': 'public',\n",
       " 'created_date': '2021-03-29T20:56:29Z',\n",
       " 'group_id': None,\n",
       " 'is_metadata_record': False}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.request(\"GET\", base_url+endpoint, headers=headers)\n",
    "data = json.loads(response.text)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response above contains a `data` json key which is of interest of us. It lists the files corresponding to the article along with their download urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'is_link_only': False,\n",
       "  'name': 'daily_rainfall_2014.png',\n",
       "  'supplied_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "  'computed_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "  'id': 26579150,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579150',\n",
       "  'size': 58863},\n",
       " {'is_link_only': False,\n",
       "  'name': 'environment.yml',\n",
       "  'supplied_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "  'computed_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "  'id': 26579171,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579171',\n",
       "  'size': 192},\n",
       " {'is_link_only': False,\n",
       "  'name': 'README.md',\n",
       "  'supplied_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "  'computed_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "  'id': 26586554,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26586554',\n",
       "  'size': 5422},\n",
       " {'is_link_only': False,\n",
       "  'name': 'data.zip',\n",
       "  'supplied_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "  'computed_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "  'id': 26766812,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766812',\n",
       "  'size': 814041183},\n",
       " {'is_link_only': False,\n",
       "  'name': 'get_data.py',\n",
       "  'supplied_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "  'computed_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "  'id': 26766815,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766815',\n",
       "  'size': 4113}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = data[\"files\"]           \n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data dump is `data.zip` which can be retreived as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the file of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.89 s, sys: 6.62 s, total: 12.5 s\n",
      "Wall time: 8min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "files_to_dl = [\"data.zip\"] \n",
    "for file in files:\n",
    "    if file[\"name\"] in files_to_dl:\n",
    "        if os.path.exists(\"rainfall/data.zip\") != True: # Do not re-download if file exists\n",
    "            os.makedirs(output_directory, exist_ok=True)\n",
    "            urlretrieve(file[\"download_url\"], output_directory + file[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "Based on running the above cell, the single act of downloading the data dump to the local machine took around 2mins 9s. This varied greately between our machines and seemed to be largely dependent on network/internet speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of the download operation across 4 different machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|           | CPU               | RAM   | HD         | Operation time |\n",
    "|-----------|-------------------|-------|------------|----------------|\n",
    "| Machine 1 | i5-4460 @ 3.20Ghz | 10 GB | 1 TB SSD   |  2 min 9 sec   |\n",
    "| Machine 2 | i7-7700HQ @2.80Ghz| 16 GB | 1 TB SSD   |  12 min 54 sec |\n",
    "| Machine 3 |  i5 @1.60Ghz      | 4 GB  | 121 GB SSD |  5 min 9 sec   |\n",
    "| Machine 4 |  i5 @2Ghz         | 16 GB | 500 GB SSD |  8 min 12 sec  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we attempt to extract the data dump we downloaded `data.zip` into individual uncompressed csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.6 s, sys: 2.9 s, total: 20.5 s\n",
      "Wall time: 21.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with zipfile.ZipFile(os.path.join(output_directory, \"data.zip\"), 'r') as f:\n",
    "    f.extractall(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of the extraction operation across 4 different machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|           | CPU               | RAM   | HD         | Operation time |\n",
    "|-----------|-------------------|-------|------------|----------------|\n",
    "| Machine 1 | i5-4460 @ 3.20Ghz | 10 GB | 1 TB SSD | 19.5 sec   |\n",
    "| Machine 2 | i7-7700HQ @ 2.80Ghz                  |     16 GB  |   1 TB SSD         |  20.3 sec              |\n",
    "| Machine 3 | i5 @1.60Ghz      | 4 GB  | 121 GB SSD |  1 min 53 sec   |\n",
    "| Machine 4 | i5 @2Ghz         | 16 GB      |  500 GB SSD          |  21.4 sec              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combining data CSVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we attempt to combine the individuals csv files into a single pandas dataframe which we then save to a single csv file called `combined_data.csv`. We create a new column called `model` to be able to identify which dataset each record originally comes from. The names of the models are extracted using regex from the csv file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 86.14 MiB, increment: 1.29 MiB\n",
      "CPU times: user 5min 4s, sys: 15.8 s, total: 5min 19s\n",
      "Wall time: 5min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit\n",
    "files = glob.glob('rainfall/*.csv')\n",
    "df = pd.concat((pd.read_csv(file, index_col=0)\n",
    "                .assign(model=re.findall(r'[^\\/]+(?=\\_d)', file)[0]) # use r'[^\\\\]+(?=\\_d)' on Windows machines\n",
    "                for file in files)\n",
    "              )\n",
    "df.to_csv(\"rainfall/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of the concatenation operation across 4 different machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|           | CPU               | RAM   | HD         | Operation time |\n",
    "|-----------|-------------------|-------|------------|----------------|\n",
    "| Machine 1 | i5-4460 @ 3.20Ghz | 10 GB | 1 TB SSD | 5 min 34 sec  |\n",
    "| Machine 2 | i7-7700HQ @ 2.80Ghz                  |     16 GB  |   1 TB SSD         |  7 min 38 sec              |\n",
    "| Machine 3 | i5 @1.60Ghz      | 4 GB  | 121 GB SSD |  Out of memory error   |\n",
    "| Machine 4 | i5 @2Ghz                  | 16 GB      | 500 GB SSD         |  5 min 25 sec              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load the combined CSV to memory and perform a simple EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the size of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.6G\trainfall/combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "du -sh rainfall/combined_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the combined dataset is 5.6 GB! We attempt to load this entire file into memory using pandas in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.4 s, sys: 9.85 s, total: 57.2 s\n",
      "Wall time: 59.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"rainfall/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62513863, 7)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>rain (mm/day)</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1889-01-01 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.244226e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1889-01-02 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.217326e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1889-01-03 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.498125e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1889-01-04 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.251282e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1889-01-05 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.270161e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time    lat_min    lat_max   lon_min   lon_max  \\\n",
       "0  1889-01-01 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "1  1889-01-02 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "2  1889-01-03 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "3  1889-01-04 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "4  1889-01-05 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "\n",
       "   rain (mm/day)            model  \n",
       "0   4.244226e-13  MPI-ESM-1-2-HAM  \n",
       "1   4.217326e-13  MPI-ESM-1-2-HAM  \n",
       "2   4.498125e-13  MPI-ESM-1-2-HAM  \n",
       "3   4.251282e-13  MPI-ESM-1-2-HAM  \n",
       "4   4.270161e-13  MPI-ESM-1-2-HAM  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "Attempting to read the above csv file results in a dead kernel in JupyterLab on some of our machines. System Resources Monitor shows a steady increase in RAM usage until 100% after which the jupyterlab notebook crashes as seen in the image below.\n",
    "\n",
    "![out-of-memory](img/machine1-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of the loading operation across 4 different machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|           | CPU               | RAM   | HD         | Operation time |\n",
    "|-----------|-------------------|-------|------------|----------------|\n",
    "| Machine 1 | i5-4460 @ 3.20Ghz | 10 GB | 1 TB SSD | OUT OF MEMORY ERROR  |\n",
    "| Machine 2 | i7-7700HQ @ 2.80Ghz                  |     16 GB  |   1 TB SSD         |  1 min 27s              |\n",
    "| Machine 3 | i5 @1.60Ghz      | 4 GB  | 121 GB SSD |  Out of memory error   |\n",
    "| Machine 4 | i5 @2Ghz                  |  16 GB     |  500 GB SSD          |  1 min 6s              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing dtype of the data\n",
    "\n",
    "Next, we attempted to change the data types of the numerical columns to reduce the memory required for storing the pandas dataframe `df` and running some simple EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time              object\n",
       "lat_min          float64\n",
       "lat_max          float64\n",
       "lon_min          float64\n",
       "lon_max          float64\n",
       "rain (mm/day)    float64\n",
       "model             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the current data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage with float64: 2500.55 MB\n",
      "Memory usage with float32: 1250.28 MB\n"
     ]
    }
   ],
   "source": [
    "# compare the memeory to store the data for different data types\n",
    "\n",
    "print(f\"Memory usage with float64: {df[['lat_min', 'lat_max','lon_min', 'lon_max', 'rain (mm/day)']].memory_usage().sum() / 1e6:.2f} MB\")\n",
    "print(f\"Memory usage with float32: {df[['lat_min', 'lat_max','lon_min', 'lon_max', 'rain (mm/day)']].astype('float32', errors='ignore').memory_usage().sum() / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            lat_min       lat_max       lon_min       lon_max  rain (mm/day)\n",
      "count  5.924854e+07  6.246784e+07  5.924854e+07  6.246784e+07   5.929456e+07\n",
      "mean  -3.310482e+01 -3.197757e+01  1.469059e+02  1.482150e+02   1.901827e+00\n",
      "std    1.963549e+00  1.992067e+00  3.793784e+00  3.809994e+00   5.588275e+00\n",
      "min   -3.646739e+01 -3.600000e+01  1.406250e+02  1.412500e+02  -3.807373e-12\n",
      "25%   -3.486911e+01 -3.366221e+01  1.434375e+02  1.450000e+02   3.876672e-06\n",
      "50%   -3.300000e+01 -3.204188e+01  1.468750e+02  1.481250e+02   6.161705e-02\n",
      "75%   -3.140170e+01 -3.015707e+01  1.501875e+02  1.513125e+02   1.021314e+00\n",
      "max   -2.990000e+01 -2.790606e+01  1.537500e+02  1.556250e+02   4.329395e+02\n",
      "peak memory: 9805.98 MiB, increment: 4392.93 MiB\n",
      "CPU times: user 14.4 s, sys: 8.52 s, total: 22.9 s\n",
      "Wall time: 24.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "print(df[['lat_min', 'lat_max','lon_min', 'lon_max', 'rain (mm/day)']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            lat_min       lat_max       lon_min       lon_max  rain (mm/day)\n",
      "count  5.924854e+07  6.246784e+07  5.924854e+07  6.246784e+07   5.929456e+07\n",
      "mean  -3.310497e+01 -3.197765e+01  1.469058e+02  1.482150e+02   1.901828e+00\n",
      "std    1.963549e+00  1.992067e+00  3.793784e+00  3.809994e+00   5.588274e+00\n",
      "min   -3.646739e+01 -3.600000e+01  1.406250e+02  1.412500e+02  -3.807373e-12\n",
      "25%   -3.486911e+01 -3.366221e+01  1.434375e+02  1.450000e+02   3.876672e-06\n",
      "50%   -3.300000e+01 -3.204189e+01  1.468750e+02  1.481250e+02   6.161705e-02\n",
      "75%   -3.140170e+01 -3.015707e+01  1.501875e+02  1.513125e+02   1.021314e+00\n",
      "max   -2.990000e+01 -2.790606e+01  1.537500e+02  1.556250e+02   4.329395e+02\n",
      "peak memory: 7636.74 MiB, increment: 3548.64 MiB\n",
      "CPU times: user 10.6 s, sys: 4.31 s, total: 14.9 s\n",
      "Wall time: 15.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "print(df[['lat_min', 'lat_max','lon_min', 'lon_max', 'rain (mm/day)']].astype('float32').describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "\n",
    "By changing the data types of the numerical columns from 'float64' to 'float32', the memory required for storing the dataframe reduces a lot. Also if we carry out the simple EDA for numerical columns using data type'float32' instead of 'float64', both time and memory usage while performing the EDA decrease as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DASK approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the limitations faced when loading the dataset using `pandas`, we attempt to use DASK - a python library that allows for parallel computing and works better with large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 5317.79 MiB, increment: 1949.67 MiB\n",
      "CPU times: user 6min 21s, sys: 25.2 s, total: 6min 46s\n",
      "Wall time: 6min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# shows time that dask take to merge\n",
    "ddf = dd.read_csv(\"rainfall/combined_data.csv\",assume_missing=True, dtype={'lon_min': 'object'})\n",
    "ddf.to_csv(\"rainfall/combined_data_dask.csv\", single_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI-ESM1-2-HR       5154240\n",
      "TaiESM1             3541230\n",
      "CMCC-CM2-HR4        3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-ESM2           3541230\n",
      "NorESM2-MM          3541230\n",
      "SAM0-UNICON         3541153\n",
      "FGOALS-f3-L         3219300\n",
      "GFDL-CM4            3219300\n",
      "GFDL-ESM4           3219300\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "MRI-ESM2-0          3037320\n",
      "BCC-CSM2-MR         3035340\n",
      "MIROC6              2070900\n",
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "INM-CM4-8           1609650\n",
      "INM-CM5-0           1609650\n",
      "KIOST-ESM           1287720\n",
      "FGOALS-g3           1287720\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "MPI-ESM1-2-LR        966420\n",
      "NESM3                966420\n",
      "AWI-ESM-1-1-LR       966420\n",
      "NorESM2-LM           919800\n",
      "CanESM5              551880\n",
      "BCC-ESM1             551880\n",
      "observed              46020\n",
      "Name: model, dtype: int64\n",
      "peak memory: 2833.30 MiB, increment: 1508.89 MiB\n",
      "CPU times: user 1min 12s, sys: 15.1 s, total: 1min 27s\n",
      "Wall time: 40.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "## count the number of records for each model\n",
    "print(ddf[\"model\"].value_counts().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               rain (mm/day)          \n",
      "                        mean       std\n",
      "model                                 \n",
      "ACCESS-CM2          1.787025  5.914188\n",
      "ACCESS-ESM1-5       2.217501  6.422397\n",
      "AWI-ESM-1-1-LR      2.026071  5.321889\n",
      "BCC-CSM2-MR         1.951832  6.200969\n",
      "CMCC-CM2-HR4        2.279350  5.629965\n",
      "peak memory: 2037.38 MiB, increment: 1456.37 MiB\n",
      "CPU times: user 1min 8s, sys: 14.3 s, total: 1min 22s\n",
      "Wall time: 35.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "## calculate the mean and std of rain amount grouping by model\n",
    "print(ddf.groupby('model').agg({'rain (mm/day)': ['mean', 'std']}).compute().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "\n",
    "DASK's use of parallel computing and optimization for large datasets comes in handy here. The machine that encountered an out of memory error when using pandas was able to successfuly laod the dataset using DASK. This suggests that we can use DASK to potentially work with larger datasets without having to increase or upgrade system resoruces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in separate chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also attempted to use pandas's chunksize argument to limit the number of lines that are read into local memory at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 5098.25 MiB, increment: 1662.06 MiB\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "import numpy as np\n",
    "rain_total = 0\n",
    "num_entries = 0\n",
    "model_counts = pd.Series(dtype='int32')\n",
    "for chunk in pd.read_csv(\"rainfall/combined_data.csv\", chunksize=10_000_000):\n",
    "    model_counts = model_counts.add(chunk[\"model\"].value_counts(),  fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ACCESS-CM2          1932840.0\n",
       "ACCESS-ESM1-5       1610700.0\n",
       "AWI-ESM-1-1-LR       966420.0\n",
       "BCC-CSM2-MR         3035340.0\n",
       "BCC-ESM1             551880.0\n",
       "CMCC-CM2-HR4        3541230.0\n",
       "CMCC-CM2-SR5        3541230.0\n",
       "CMCC-ESM2           3541230.0\n",
       "CanESM5              551880.0\n",
       "EC-Earth3-Veg-LR    3037320.0\n",
       "FGOALS-f3-L         3219300.0\n",
       "FGOALS-g3           1287720.0\n",
       "GFDL-CM4            3219300.0\n",
       "GFDL-ESM4           3219300.0\n",
       "INM-CM4-8           1609650.0\n",
       "INM-CM5-0           1609650.0\n",
       "KIOST-ESM           1287720.0\n",
       "MIROC6              2070900.0\n",
       "MPI-ESM-1-2-HAM      966420.0\n",
       "MPI-ESM1-2-HR       5154240.0\n",
       "MPI-ESM1-2-LR        966420.0\n",
       "MRI-ESM2-0          3037320.0\n",
       "NESM3                966420.0\n",
       "NorESM2-LM           919800.0\n",
       "NorESM2-MM          3541230.0\n",
       "SAM0-UNICON         3541153.0\n",
       "TaiESM1             3541230.0\n",
       "observed              46020.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "By using chunksize argument of `read_csv` we are able to work-around the memory limitation we previously experienced with `pandas` loading of large datasets. Here, for example, we were able to calculate the average rain fall per day across all days included in the data dump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading particular columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also attempted to only load some columns that could be the more important for our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI-ESM1-2-HR       5154240\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-CM2-HR4        3541230\n",
      "NorESM2-MM          3541230\n",
      "CMCC-ESM2           3541230\n",
      "TaiESM1             3541230\n",
      "SAM0-UNICON         3541153\n",
      "FGOALS-f3-L         3219300\n",
      "GFDL-ESM4           3219300\n",
      "GFDL-CM4            3219300\n",
      "MRI-ESM2-0          3037320\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "BCC-CSM2-MR         3035340\n",
      "MIROC6              2070900\n",
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "INM-CM4-8           1609650\n",
      "INM-CM5-0           1609650\n",
      "FGOALS-g3           1287720\n",
      "KIOST-ESM           1287720\n",
      "NESM3                966420\n",
      "AWI-ESM-1-1-LR       966420\n",
      "MPI-ESM1-2-LR        966420\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "NorESM2-LM           919800\n",
      "BCC-ESM1             551880\n",
      "CanESM5              551880\n",
      "observed              46020\n",
      "Name: model, dtype: int64\n",
      "peak memory: 7082.89 MiB, increment: 1832.58 MiB\n",
      "CPU times: user 52.1 s, sys: 11.9 s, total: 1min 4s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "use_cols = [\"time\", \"lat_min\", \"lat_max\", \"lon_min\", \"lon_max\", \"rain (mm/day)\", \"model\"]\n",
    "df = pd.read_csv(\"rainfall/combined_data.csv\", usecols=use_cols)\n",
    "print(df[\"model\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI-ESM1-2-HR       5154240\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-CM2-HR4        3541230\n",
      "NorESM2-MM          3541230\n",
      "CMCC-ESM2           3541230\n",
      "TaiESM1             3541230\n",
      "SAM0-UNICON         3541153\n",
      "FGOALS-f3-L         3219300\n",
      "GFDL-ESM4           3219300\n",
      "GFDL-CM4            3219300\n",
      "MRI-ESM2-0          3037320\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "BCC-CSM2-MR         3035340\n",
      "MIROC6              2070900\n",
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "INM-CM4-8           1609650\n",
      "INM-CM5-0           1609650\n",
      "FGOALS-g3           1287720\n",
      "KIOST-ESM           1287720\n",
      "NESM3                966420\n",
      "AWI-ESM-1-1-LR       966420\n",
      "MPI-ESM1-2-LR        966420\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "NorESM2-LM           919800\n",
      "BCC-ESM1             551880\n",
      "CanESM5              551880\n",
      "observed              46020\n",
      "Name: model, dtype: int64\n",
      "peak memory: 6276.19 MiB, increment: 2917.65 MiB\n",
      "CPU times: user 39.7 s, sys: 5.54 s, total: 45.3 s\n",
      "Wall time: 46.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "use_cols = [\"time\", \"model\"]\n",
    "df = pd.read_csv(\"rainfall/combined_data.csv\", usecols=use_cols)\n",
    "print(df[\"model\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "\n",
    "By only loading some columns that are directly related to the process of interest (in this case counting how many data point of each model), we are able to reduce the memory and time requirement for this process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Perform a simple EDA in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer dataframe to R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to transfer the dataframe to R we will attempt to use `parquet` - a protable language-agnostic file format for storing dataframes from the Apache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will use the `datasets` module from the `pyarrow` library to create a memory-efficient representation of the combined `csv` file. Arrow represetnation are columnar and memory efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "dataset = ds.dataset(\"rainfall/combined_data.csv\", format=\"csv\")\n",
    "arrow_table = dataset.to_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can write the `Arrow` table to a `parquet` file on disk as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "pq.write_table(arrow_table, 'rainfall/combined_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen below, the size of the resulting `parquet` file is much smaller compared to the combined `csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "542M\trainfall/combined_data.parquet\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "du -sh rainfall/combined_data.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In R, we can utility the `read_parquet` function from `arrow` package to load the file into memory and create an R dataframe. From there, we perform some basic EDA by counting the records we have for each model in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: \n",
      "Attaching package: ‘arrow’\n",
      "\n",
      "\n",
      "R[write to console]: The following object is masked from ‘package:utils’:\n",
      "\n",
      "    timestamp\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: \n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m# A tibble: 28 x 2\u001b[39m\n",
      "   model                  n\n",
      "   \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m              \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m\n",
      "\u001b[90m 1\u001b[39m ACCESS-CM2       1\u001b[4m9\u001b[24m\u001b[4m3\u001b[24m\u001b[4m2\u001b[24m840\n",
      "\u001b[90m 2\u001b[39m ACCESS-ESM1-5    1\u001b[4m6\u001b[24m\u001b[4m1\u001b[24m\u001b[4m0\u001b[24m700\n",
      "\u001b[90m 3\u001b[39m AWI-ESM-1-1-LR    \u001b[4m9\u001b[24m\u001b[4m6\u001b[24m\u001b[4m6\u001b[24m420\n",
      "\u001b[90m 4\u001b[39m BCC-CSM2-MR      3\u001b[4m0\u001b[24m\u001b[4m3\u001b[24m\u001b[4m5\u001b[24m340\n",
      "\u001b[90m 5\u001b[39m BCC-ESM1          \u001b[4m5\u001b[24m\u001b[4m5\u001b[24m\u001b[4m1\u001b[24m880\n",
      "\u001b[90m 6\u001b[39m CanESM5           \u001b[4m5\u001b[24m\u001b[4m5\u001b[24m\u001b[4m1\u001b[24m880\n",
      "\u001b[90m 7\u001b[39m CMCC-CM2-HR4     3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m 8\u001b[39m CMCC-CM2-SR5     3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m 9\u001b[39m CMCC-ESM2        3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m10\u001b[39m EC-Earth3-Veg-LR 3\u001b[4m0\u001b[24m\u001b[4m3\u001b[24m\u001b[4m7\u001b[24m320\n",
      "\u001b[90m# … with 18 more rows\u001b[39m\n",
      "Time difference of 7.321429 secs\n",
      "CPU times: user 8.69 s, sys: 3.45 s, total: 12.1 s\n",
      "Wall time: 7.75 s\n"
     ]
    }
   ],
   "source": [
    "## Code adapted from course material for DSCI525 at UBC 2020\n",
    "%%time\n",
    "%%R\n",
    "library(arrow)\n",
    "start_time <- Sys.time()\n",
    "r_data <- read_parquet(\"rainfall/combined_data.parquet\")\n",
    "print(class(r_data))\n",
    "library(dplyr)\n",
    "result <- r_data %>% \n",
    "    count(model)\n",
    "end_time <- Sys.time()\n",
    "print(result)\n",
    "print(end_time - start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rationale for choosing this approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use `parquet` file as the mode of exchange between Python and R. `parquet` was choosen for a number of reasons. First, `parquet` files on disk take up less space when compared to conventional file formats such as `csv`. Even compared to the Arrow-based file format `feather`, `parquet` file was approxiamtely 50% smaller than `feather` file for this dataset. (542MB vs 1.1GB). The other reason `parquet` was chosen is performance. Loading and reading of `parquet` files into memory is efficient in both languages, Python and R. As seen in our R test above, loading the dataset into memory and performing some basic EDA took approximately 7.75 seconds. This result can be contrasted with loading times observed when loading the same dataset in `csv` format in Python (fastest time was greater than 60 seconds). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
